# Model Inventory
<br>

## These models are open-source with clear licenses. The models are also tested using ComfyUI

| Models | Base Model | Params | Vram requirement | Licencse |
|---|---|---|---|---|
| Stable Diffusion v1.5 | SDL 1.5 | 860M | 6gb | OpenRail-M |
| Stable Diffusion XL | SDXL 1.0 | 2.6B | 8gb | OpenRail-M |
| Juggernaut XL | Fine tuned SDXL 1.0 | 2.6B | 8gb | OpenRail-M |
| Flux 1 schnell | SDL 1.5 | 12B | 12gb | Apache 2.0 |

<br>

## Evaluation Setup (ComfyUI)
| Parameter     | Value |
|---------------|-------|
| Resolution    | 512×512 and 1024×1024 |
| Steps         | 20 |
| Sampler       | Euler |
| Batch Sizes   | 1, 4, 8 |
| Total Prompts | 10 |

---

<br>

# Benchmark Rubric – Quality


## Quality Metric

| Metric Used | Definition | Scoring | Limitations |
|-------------|------------|---------|-------------|
| **CLIPScore** | Measures how well a generated image semantically matches its text prompt using OpenAI CLIP embeddings. Higher values indicate stronger alignment. | - Compute CLIPScore per image.<br>- Report the **mean CLIPScore** across all prompts for each model. | - Does not measure realism or aesthetics.<br>- Sensitive to prompt wording and CLIP training biases. |

<br> 

We calculate the **mean CLIPScore** for each model by first running our CLIPScore script on all images generated by that model.  
The script evaluates every image–prompt pair and outputs a CLIPScore (semantic alignment score).
We then take the **average score per model** (across 10 prompts) to represent its overall quality performance.  

---

<br>

# Benchmark Rubric – Speed

<br>

## Speed Metric

| Metric Used | Definition | Scoring | Limitations |
|-------------|------------|---------|-------------|
| **Latency (ms/img)** | Time required to generate a single image at batch size = 1. | - Record elapsed time of execution from ComfyUI (terminal logs).<br>- Average across all prompts.<br>- Lower is better. | Results depend on GPU hardware and prompt complexity. |
| **Throughput (imgs/min)** | Number of images produced per minute at batch sizes 1, 4, and 8. | - Measure elapsed execution time for generating all images (from terminal logs).<br>- Convert to images/min.<br>- Higher is better. | Strongly affected by VRAM limits and model parallelism.<br>- Not directly comparable across different hardware. |

---

<br>


## Memory Metric

| Metric Used | Definition | Scoring | Limitations |
|-------------|------------|---------|-------------|
| **VRAM Load (GB)** | The minimum GPU memory allocated when loading the model into memory before generation begins. | - Record VRAM allocation shown in ComfyUI terminal at model load.<br>- Cross-check with `nvidia-smi`.<br>- Lower values indicate lighter models with broader hardware compatibility. | May differ slightly between ComfyUI logs and `nvidia-smi` readings due to driver reporting. |
| **Peak VRAM (GB)** | The maximum GPU memory consumption during image generation. | - Observe VRAM usage reported in ComfyUI terminal during generation at 512×512 and 1024×1024 resolutions, across batch sizes 1, 4, and 8.<br>- Confirm with `nvidia-smi` monitoring.<br>- Lower values indicate greater efficiency. | Peak usage is influenced by resolution, batch size, and sampler.<br>- GPU background processes may introduce fluctuations. |

<br>

# Benchmark Report Template

## 1. Introduction
This benchmark evaluates open-source text-to-image models under consistent conditions.  
Models are assessed across three dimensions: **Quality**, **Speed**, and **Memory**.  
The evaluation dataset consists of **10 fixed prompts** (`prompts.txt`).  
All experiments are conducted using **ComfyUI** on a single GPU.

---

## 3. Quality Results (CLIPScore)
We compute CLIPScore for each generated image and report the **mean CLIPScore per model**.

| Model | Mean CLIPScore |
|-------|----------------|
| Stable Diffusion v1.5 | – |
| Stable Diffusion XL   | – |
| Juggernaut XL         | – |
| Flux 1 Schnell        | – |

---

## 4. Speed Results (Execution Time)
Measured using **elapsed time of execution** from ComfyUI terminal logs.

| Model | Resolution | Batch Size | Latency (ms/img) | Throughput (imgs/min) |
|-------|------------|------------|------------------|------------------------|
| Stable Diffusion v1.5 | 512×512 | 1 | – | – |
| Stable Diffusion v1.5 | 512×512 | 4 | – | – |
| Stable Diffusion XL   | 1024×1024 | 1 | – | – |
| Juggernaut XL         | 512×512 | 1 | – | – |
| Flux 1 Schnell        | 512×512 | 8 | – | – |

---

## 5. Memory Results (VRAM Usage)
Measured from **ComfyUI terminal logs** and verified with `nvidia-smi`.

| Model | Resolution | Batch Size | VRAM Load (GB) | Peak VRAM (GB) |
|-------|------------|------------|----------------|----------------|
| Stable Diffusion v1.5 | 512×512 | 1 | – | – |
| Stable Diffusion XL   | 1024×1024 | 1 | – | – |
| Juggernaut XL         | 512×512 | 4 | – | – |
| Flux 1 Schnell        | 512×512 | 8 | – | – |

---

⚠️ **This draft is subject to review and further refinement.**


